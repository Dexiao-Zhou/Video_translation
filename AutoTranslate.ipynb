{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /private/var/folders/pz/qq52dgsj7dn690q5b3f2nb440000gn/T/pip-req-build-z57sd01n\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /private/var/folders/pz/qq52dgsj7dn690q5b3f2nb440000gn/T/pip-req-build-z57sd01n\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting numba (from openai-whisper==20231117)\n",
      "  Using cached numba-0.60.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting numpy (from openai-whisper==20231117)\n",
      "  Using cached numpy-2.1.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting torch (from openai-whisper==20231117)\n",
      "  Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting tqdm (from openai-whisper==20231117)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting more-itertools (from openai-whisper==20231117)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper==20231117)\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.16.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper==20231117)\n",
      "  Using cached llvmlite-0.43.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting numpy (from openai-whisper==20231117)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting sympy (from torch->openai-whisper==20231117)\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->openai-whisper==20231117)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch->openai-whisper==20231117)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting setuptools (from torch->openai-whisper==20231117)\n",
      "  Using cached setuptools-74.1.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->openai-whisper==20231117)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->openai-whisper==20231117)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl (279 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached numba-0.60.0-cp312-cp312-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl (906 kB)\n",
      "Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached idna-3.8-py3-none-any.whl (66 kB)\n",
      "Using cached llvmlite-0.43.0-cp312-cp312-macosx_11_0_arm64.whl (28.8 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached setuptools-74.1.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802824 sha256=f99bfdc8da36265894c11d930e91b60b738493f2bc768c3dd13930c8fdd26787\n",
      "  Stored in directory: /private/var/folders/pz/qq52dgsj7dn690q5b3f2nb440000gn/T/pip-ephem-wheel-cache-i8t1n0tj/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: sentencepiece, mpmath, urllib3, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, numpy, networkx, more-itertools, MarkupSafe, llvmlite, idna, fsspec, filelock, charset-normalizer, certifi, requests, numba, jinja2, torch, tiktoken, huggingface-hub, tokenizers, openai-whisper, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.0 fsspec-2024.9.0 huggingface-hub-0.24.6 idna-3.8 jinja2-3.1.4 llvmlite-0.43.0 more-itertools-10.5.0 mpmath-1.3.0 networkx-3.3 numba-0.60.0 numpy-2.0.2 openai-whisper-20231117 pyyaml-6.0.2 regex-2024.7.24 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 setuptools-74.1.2 sympy-1.13.2 tiktoken-0.7.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: German\n",
      "[00:00.000 --> 00:04.760]  Das hängt von ihren Noten ab und machen unterschiedliche Schulabschlüsse.\n",
      "[00:04.760 --> 00:07.240]  Der höchste Schulabschluss ist das Abitur.\n",
      "[00:07.240 --> 00:09.440]  Damit kann man an einer Universität schlafen.\n",
      "SRT 文件已保存至：/Users/zhoudexiao/Desktop/Project/testaudio/segment-7-f2-v1-a.srt\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import whisper\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "def generate_srt_from_audio(audio_file_path, output_srt_file_path):\n",
    "    \"\"\"\n",
    "    从音频文件生成 SRT 字幕文件。\n",
    "\n",
    "    参数:\n",
    "    audio_file_path (str): 音频文件的路径。\n",
    "    output_srt_file_path (str): 输出 SRT 文件的路径。\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    # model = whisper.load_model(\"large-v2\")\n",
    "    model = whisper.load_model(\"medium\")\n",
    "\n",
    "    # 转录音频文件\n",
    "    result = model.transcribe(audio_file_path, verbose=True)\n",
    "\n",
    "    # 将转录结果转换为 SRT 格式并保存\n",
    "    with open(output_srt_file_path, 'w', encoding='UTF-8') as srt_file:\n",
    "        for i, segment in enumerate(result[\"segments\"], start=1):\n",
    "            start_seconds = segment[\"start\"]\n",
    "            end_seconds = segment[\"end\"]\n",
    "            # 格式化开始和结束时间为 SRT 规范\n",
    "            start_srt = str(timedelta(seconds=start_seconds)).replace('.', ',')\n",
    "            end_srt = str(timedelta(seconds=end_seconds)).replace('.', ',')\n",
    "            # 确保毫秒是三位数字\n",
    "            if ',' not in start_srt:\n",
    "                start_srt += ',000'\n",
    "            if ',' not in end_srt:\n",
    "                end_srt += ',000'\n",
    "            text = segment[\"text\"]\n",
    "            srt_file.write(f\"{i}\\n\")\n",
    "            srt_file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "    print(f\"SRT 文件已保存至：{output_srt_file_path}\")\n",
    "\n",
    "# 音频文件路径列表\n",
    "audio_folder_path = '/Users/zhoudexiao/Desktop/Project/testaudio/'\n",
    "audio_files = [\n",
    "    'segment-7-f2-v1-a.mp3'\n",
    "]\n",
    "\n",
    "# 遍历音频文件路径列表并生成对应的 SRT 文件\n",
    "for audio_file in audio_files:\n",
    "    audio_file_path = os.path.join(audio_folder_path, audio_file)\n",
    "    # 获取音频文件名，不包括路径和扩展名\n",
    "    audio_name = audio_file.split('.')[0]\n",
    "    # 设置 SRT 输出文件路径\n",
    "    output_srt_file_path = os.path.join(audio_folder_path, f'{audio_name}.srt')\n",
    "    # 生成 SRT 文件\n",
    "    generate_srt_from_audio(audio_file_path, output_srt_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:04.760]  Das hängt von ihren Noten ab und machen unterschiedliche Schulabschlüsse.\n",
      "[00:04.760 --> 00:07.240]  Der höchste Schulabschluss ist das Abitur.\n",
      "[00:07.240 --> 00:09.440]  Damit kann man an einer Universität schlafen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating text:  Das hängt von ihren Noten ab und machen unterschiedliche Schulabschlüsse.\n",
      "Translated text: 那得看他们的成绩 学历不同\n",
      "Translating text:  Der höchste Schulabschluss ist das Abitur.\n",
      "Translated text: 高中毕业典礼是毕业典礼\n",
      "Translating text:  Damit kann man an einer Universität schlafen.\n",
      "Translated text: 你可以睡在大学里\n",
      "SRT 文件已保存至：/Users/zhoudexiao/Desktop/Project/testaudio/segment-7-f2-v1-a.srt\n"
     ]
    }
   ],
   "source": [
    "# 直接模型翻译\n",
    "\n",
    "import whisper\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def generate_translated_srt(audio_file_path, output_srt_file_path, source_lang=\"de\", target_lang=\"zh\"):\n",
    "    \"\"\"\n",
    "    从音频文件生成带翻译的 SRT 字幕文件。\n",
    "\n",
    "    参数:\n",
    "    audio_file_path (str): 音频文件的路径。\n",
    "    output_srt_file_path (str): 输出 SRT 文件的路径。\n",
    "    source_lang (str): 源语言代码。\n",
    "    target_lang (str): 目标语言代码。\n",
    "    \"\"\"\n",
    "    # 加载 Whisper 模型\n",
    "    # whisper_model = whisper.load_model(\"large-v3\")\n",
    "    whisper_model = whisper.load_model(\"medium\")\n",
    "\n",
    "    # 转录音频文件\n",
    "    result = whisper_model.transcribe(audio_file_path, language=source_lang, verbose=True)\n",
    "\n",
    "    # 加载翻译模型和 tokenizer\n",
    "    translation_model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "    translation_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "\n",
    "    # 将转录结果转换为 SRT 格式并保存\n",
    "    with open(output_srt_file_path, 'w', encoding='UTF-8') as srt_file:\n",
    "        for i, segment in enumerate(result[\"segments\"], start=1):\n",
    "            start_seconds = segment[\"start\"]\n",
    "            end_seconds = segment[\"end\"]\n",
    "            # 格式化开始和结束时间为 SRT 规范\n",
    "            start_srt = str(timedelta(seconds=start_seconds)).replace('.', ',')\n",
    "            end_srt = str(timedelta(seconds=end_seconds)).replace('.', ',')\n",
    "            # 确保毫秒是三位数字\n",
    "            if ',' not in start_srt:\n",
    "                start_srt += ',000'\n",
    "            if ',' not in end_srt:\n",
    "                end_srt += ',000'\n",
    "            text = segment[\"text\"]\n",
    "\n",
    "            # 翻译文本\n",
    "            print(f\"Translating text: {text}\")  # 调试信息\n",
    "            inputs = tokenizer.encode(text, return_tensors='pt', padding=True)\n",
    "            translated_tokens = translation_model.generate(inputs, max_length=400, num_beams=4, early_stopping=True)\n",
    "            translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            print(f\"Translated text: {translated_text}\")  # 调试信息\n",
    "\n",
    "# 使用中介语言进行优化\n",
    "\n",
    "            # 写入 SRT 文件\n",
    "            srt_file.write(f\"{i}\\n\")\n",
    "            srt_file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
    "            srt_file.write(f\"{translated_text}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "    print(f\"SRT 文件已保存至：{output_srt_file_path}\")\n",
    "\n",
    "# 音频文件路径列表\n",
    "audio_folder_path = '/Users/zhoudexiao/Desktop/Project/testaudio/'\n",
    "audio_files = [\n",
    "    'segment-7-f2-v1-a.mp3'\n",
    "    # 添加其他音频文件名\n",
    "]\n",
    "\n",
    "# 遍历音频文件路径列表并生成对应的 SRT 文件\n",
    "for audio_file in audio_files:\n",
    "    audio_file_path = os.path.join(audio_folder_path, audio_file)\n",
    "    # 获取音频文件名，不包括路径和扩展名\n",
    "    audio_name = os.path.splitext(audio_file)[0]\n",
    "    # 设置 SRT 输出文件路径\n",
    "    output_srt_file_path = os.path.join(audio_folder_path, f'{audio_name}.srt')\n",
    "    # 生成 SRT 文件\n",
    "    generate_translated_srt(audio_file_path, output_srt_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:04.760]  Das hängt von ihren Noten ab und machen unterschiedliche Schulabschlüsse.\n",
      "[00:04.760 --> 00:07.240]  Der höchste Schulabschluss ist das Abitur.\n",
      "[00:07.240 --> 00:09.440]  Damit kann man an einer Universität schlafen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/Users/zhoudexiao/Desktop/Project/testaudio/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1:  Das hängt von ihren Noten ab und machen unterschiedliche Schulabschlüsse. -> 取决于他们的成绩,成绩不同。\n",
      "Segment 2:  Der höchste Schulabschluss ist das Abitur. -> 最高学校结业证书是Abitur。\n",
      "Segment 3:  Damit kann man an einer Universität schlafen. -> 你可以在大学里用它睡觉\n",
      "SRT 文件已保存至：/Users/zhoudexiao/Desktop/Project/testaudio/segment-7-f2-v1-a.srt\n"
     ]
    }
   ],
   "source": [
    "# 英语作为中间语言翻译\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import whisper\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def generate_translated_srt(audio_file_path, output_srt_file_path, source_lang=\"de\", target_lang=\"zh\"):\n",
    "    \"\"\"\n",
    "    从音频文件生成带翻译的 SRT 字幕文件。\n",
    "    \"\"\"\n",
    "    # 加载 Whisper 模型\n",
    "    whisper_model = whisper.load_model(\"medium\")\n",
    "    # whisper_model = whisper.load_model(\"large\")\n",
    "    # whisper_model = whisper.load_model(\"small\")\n",
    "\n",
    "    # 转录音频文件\n",
    "    result = whisper_model.transcribe(\n",
    "        audio_file_path,\n",
    "        language=source_lang,\n",
    "        verbose=True,\n",
    "        no_speech_threshold=0.6,\n",
    "        condition_on_previous_text = True\n",
    "        # beam_size=5,\n",
    "        # temperature=0.7\n",
    "    )\n",
    "\n",
    "    # 加载翻译模型和 tokenizer，首先将源语言翻译成英语\n",
    "    intermediate_lang = \"en\"\n",
    "    translation_model_intermediate = f'Helsinki-NLP/opus-mt-{source_lang}-{intermediate_lang}'\n",
    "    tokenizer_intermediate = MarianTokenizer.from_pretrained(translation_model_intermediate)\n",
    "    model_intermediate = MarianMTModel.from_pretrained(translation_model_intermediate)\n",
    "\n",
    "    # 再将英语翻译成目标语言\n",
    "    translation_model_final = f'Helsinki-NLP/opus-mt-{intermediate_lang}-{target_lang}'\n",
    "    tokenizer_final = MarianTokenizer.from_pretrained(translation_model_final)\n",
    "    model_final = MarianMTModel.from_pretrained(translation_model_final)\n",
    "\n",
    "    # 将转录结果转换为 SRT 格式并保存\n",
    "    with open(output_srt_file_path, 'w', encoding='UTF-8') as srt_file:\n",
    "        for i, segment in enumerate(result[\"segments\"], start=1):\n",
    "            start_seconds = segment[\"start\"]\n",
    "            end_seconds = segment[\"end\"]\n",
    "            start_srt = str(timedelta(seconds=start_seconds)).replace('.', ',')\n",
    "            end_srt = str(timedelta(seconds=end_seconds)).replace('.', ',')\n",
    "            if ',' not in start_srt:\n",
    "                start_srt += ',000'\n",
    "            if ',' not in end_srt:\n",
    "                end_srt += ',000'\n",
    "            text = segment[\"text\"]\n",
    "\n",
    "            # 先翻译到英语\n",
    "            inputs = tokenizer_intermediate.encode(text, return_tensors='pt', padding=True)\n",
    "            intermediate_tokens = model_intermediate.generate(inputs, max_length=400, num_beams=4, early_stopping=True)\n",
    "            intermediate_text = tokenizer_intermediate.decode(intermediate_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            # 再从英语翻译到目标语言\n",
    "            inputs_final = tokenizer_final.encode(intermediate_text, return_tensors='pt', padding=True)\n",
    "            final_tokens = model_final.generate(inputs_final, max_length=400, num_beams=4, early_stopping=True)\n",
    "            translated_text = tokenizer_final.decode(final_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            srt_file.write(f\"{i}\\n\")\n",
    "            srt_file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
    "            srt_file.write(f\"{translated_text}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "            # 调试输出翻译文本\n",
    "            print(f\"Segment {i}: {text} -> {translated_text}\")\n",
    "        print(f\"SRT 文件已保存至：{output_srt_file_path}\")\n",
    "\n",
    "# 设置音频文件路径和输出\n",
    "# audio_folder_path = './drive/MyDrive/Colab Notebooks/KI2/'\n",
    "audio_folder_path = '/Users/zhoudexiao/Desktop/Project/testaudio/'\n",
    "audio_files = [\n",
    "               'segment-7-f2-v1-a.mp3'\n",
    "            #    'Kardiologische Implantate Teil 2 Woche 10.mp3',\n",
    "            #    'Kardiologische Implantate Teil 2 Woche 11.mp3',\n",
    "            #    'Kardiologische Implantate Teil 2 Woche 12.mp3'\n",
    "]\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    audio_file_path = os.path.join(audio_folder_path, audio_file)\n",
    "    audio_name = os.path.splitext(audio_file)[0]\n",
    "    output_srt_file_path = os.path.join(audio_folder_path, f'{audio_name}.srt')\n",
    "    generate_translated_srt(audio_file_path, output_srt_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
